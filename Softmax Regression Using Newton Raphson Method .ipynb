{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Multinomial Logistic Regression - Soft Max Regression\n",
    "#### Using Newton-Raphson Method : Quasi-Newton Methods\n",
    "#### Notebook Author: Nirupam Purushothama\n",
    "\n",
    "#### Referred Books & Links:\n",
    "1. Machine Learning - A Probabilistic Perspective - Kevin P. Murphy\n",
    "2. Pattern Recognition and Machine Learning - Christopher M. Bishop\n",
    "3. Hands-on Machine Learning with Scikit-Learn & TensorFlow - Aurelien Geron\n",
    "4. [Online - Lecture1](http://www.stat.cmu.edu/~ryantibs/convexopt-S15/lectures/14-newton.pdf)\n",
    "5. [Online - Lecture2](https://www.stat.cmu.edu/~ryantibs/convexopt/lectures/quasi-newton.pdf)\n",
    "6. [BFGS - Wiki](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\n",
    "7. [Quasi-Newton - Wiki](https://en.wikipedia.org/wiki/Quasi-Newton_method)\n",
    "8. [Backtracking Line Search - Wiki](https://en.wikipedia.org/wiki/Backtracking_line_search)\n",
    "\n",
    "Despite this effort, I expect bugs/errors to be present. You are more than welcome to report them. Will be happy to fix them. This is not intended to be reusable library code. It is just for academic/learning purposes. The reason why this had to be bridged from so many sources is because none of the locations had a clear description of the algorithm to directly translate that into code. Hence, had to glean it from multiple sources.\n",
    "\n",
    "This notebook doesn't use Sci-kit learn's funciton but codes the Newton Raphson method. So, for those interested in the internal details of descending a Multinomial Regression's Negative Likelihood Function to find the final set of parameters check the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./training.csv\")\n",
    "d_max = train_df.shape[1]\n",
    "\n",
    "x = np.array(train_df.iloc[:,1:d_max])\n",
    "y = np.array(train_df.iloc[:,0])\n",
    "\n",
    "# To adjust for the code below\n",
    "y = y -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. The softmax score function is defined below:\n",
    "The function calculate_mu computes the estimated probability that x belongs to class k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial logistic regression\n",
    "# theta_lcl - D X C matrix\n",
    "# x_lcl - N x D matrix\n",
    "# returns Mu - N x C Matrix\n",
    "# Where c = #Categories, D = #Features\n",
    "def calculate_mu(x_lcl, theta_lcl):\n",
    "    \n",
    "    # Calculate the score function\n",
    "    scores = np.matmul(x_lcl,theta_lcl)\n",
    "    \n",
    "    # Convert to a manageable range\n",
    "    min_l = np.min(scores)\n",
    "    max_l = np.max(scores)\n",
    "    o_range = max_l - min_l\n",
    "    n_range = 600\n",
    "    \n",
    "    scores = (scores - min_l) * n_range / o_range\n",
    "    \n",
    "    exps = np.exp(scores)\n",
    "    #exps = np.abs(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exp_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a y_hot which is a N x C matrix\n",
    "def to_one_hot(y):\n",
    "    n_classes = y.max()+1\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, n_classes))\n",
    "    Y_one_hot[np.arange(m), y] = 1\n",
    "    \n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = to_one_hot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Writing the function that reports the accuracy of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(the):\n",
    "    test_df = pd.read_csv(\"./testing.csv\")\n",
    "    d_max = test_df.shape[1]\n",
    "    x_t = np.array(test_df.iloc[:,1:d_max])\n",
    "    y_t = np.array(test_df.iloc[:,0])\n",
    "\n",
    "    # To adjust for the code below\n",
    "    y_t = y_t -1\n",
    "\n",
    "    x_t.shape[1]\n",
    "\n",
    "\n",
    "    m =  x_t.shape[0] # number of data points\n",
    "\n",
    "    # Add a bias to each of the rows (i.e. a constant)\n",
    "    x_t_b = np.c_[np.ones((m,1)),x_t]\n",
    "\n",
    "\n",
    "    logs = x_t_b.dot(the)\n",
    "    Y_proba = calculate_mu(x_t_b, the)\n",
    "    y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "    accuracy_score = np.mean(y_predict == y_t)\n",
    "    return accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Gradient Calculation\n",
    "\n",
    "g(W) = ∇f(w) = $\\sum_{i=1}^{N}(μ_i - y_i) ⊗ x_i$\n",
    "\n",
    "Where \n",
    "* w = $\\theta$\n",
    "* μ = prob that this point belongs to category (notations differ between textbooks)\n",
    "* n = m = i = data point (notations differ between textbooks)\n",
    "* $\\varphi$ = x \n",
    "* I = Identity matrix\n",
    "* y = the y_hot vector we defined above\n",
    "\n",
    "⊗ is [Kronecker Product](https://en.wikipedia.org/wiki/Kronecker_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a CD x 1 vector \n",
    "# mu_lcl - N x C Matrix\n",
    "# y_lcl - N x C Matrix - y_hot matrix\n",
    "# x_lcl - N x D matrix\n",
    "# Where c = #Categories, D = #Features\n",
    "def gradient(mu_lcl, y_lcl, x_lcl):\n",
    "    # The N rows\n",
    "    N = x_lcl.shape[0]\n",
    "    # C Categories\n",
    "    C = mu_lcl.shape[1]\n",
    "    # D Dimensions\n",
    "    D = x_lcl.shape[1]\n",
    "    \n",
    "    g = np.zeros((C*D,1))\n",
    "    \n",
    "    for i in range(N):\n",
    "        diff = mu_lcl[i:i+1] - y_lcl[i:i+1]\n",
    "\n",
    "        # diff.T is a C x 1 vector and x[i:i+1].T is a D x 1 vector\n",
    "        # resultant cron multiplication is a CD x 1 vector\n",
    "        kron_result = np.kron(diff.T,x_lcl[i:i+1].T)\n",
    "        \n",
    "        g = g + kron_result\n",
    "        \n",
    "    return g   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Direct Hessian Computation\n",
    "Considered two text books and both had different notations. Hence summarizing the notations as follows:\n",
    "\n",
    "Books referred were:\n",
    "Pattern Recognition and Machine Learning - Bishop &\n",
    "\n",
    "Machine Learning - A Probabilistic Perspective - Murphy\n",
    "\n",
    "Hessian is given by \n",
    "$∇w_k∇w_j  E(w_1, . . . ,w_K)$ = $−\\sum_{n=1}^{N}$ $y_n^k$ $(I_k^j - y_n^j)$ $φ_nφ_n^T$\n",
    "\n",
    "##### Writing this in a condensed format this is will be \n",
    "\n",
    "H(W) = $∇^2 f(w) = \\sum_{i=1}^{N}(diag(μ_i) − μ_iμ_i^T) ⊗ (x_i x_i^T)$\n",
    "\n",
    "Where \n",
    "* w = $\\theta$\n",
    "* μ = prob that this point belongs to category (notations differ between textbooks)\n",
    "* n = m = i = data point (notations differ between textbooks)\n",
    "* $\\varphi$ = x \n",
    "* I = Identity matrix\n",
    "\n",
    "⊗ is Kronecker Product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mu is a N x C Matrix\n",
    "# x is a N x D Matrix\n",
    "# Hessian is a CD x CD Matrix\n",
    "def hessian(mu, x):\n",
    "    # The N rows\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    C = mu.shape[1]\n",
    "    D = x.shape[1]\n",
    "    \n",
    "    # size of hessian is C*D X C*D [Where c = #Categories, D = #Features]\n",
    "    h = np.zeros((C*D,C*D))\n",
    "    \n",
    "    # i is the ith data point\n",
    "    for i in range(N):\n",
    "        \n",
    "        # d_mu_i is a C x C Matrix\n",
    "        d_mu_i = np.diagflat(mu[i])\n",
    "\n",
    "        # mu_sq C x C Matrix\n",
    "        # Here for us the array is provided as a row vector and hence we do array.T * array.         \n",
    "        # whereas in the definition above it is vector * vector.T . Notice that difference\n",
    "        mu_sq = np.matmul(mu[i:i+1].T, mu[i:i+1])\n",
    "    \n",
    "        # xi_sq is a D x D Matrix\n",
    "        xi_sq = np.matmul(x[i:i+1].T, x[i:i+1])\n",
    "        \n",
    "        # Kron-result is a CD x CD Matrix\n",
    "        kron_result = np.kron((d_mu_i - mu_sq),xi_sq)\n",
    "        \n",
    "        # Summate across all data points\n",
    "        h = h + kron_result\n",
    "        \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Quasi-Newton (Metric) Methods - Hessian and Inverse Calculation\n",
    "\n",
    "<b>Reference: Machine Learning - A probabilistic perspective - Murphy</b>\n",
    "\n",
    "Sometimes H goes into singularities leading to no progress in the algorithm. And also H may be too expensive to compute. Quasi-Newton methods iteratively build up an approximation to the Hessian using information gleaned\n",
    "from the gradient vector at each step. The most common method is called BFGS (named after its inventors, Broyden, Fletcher, Goldfarb and Shanno), which updates the approximation to the Hessian $B_k ≈ H_k$ as follows:\n",
    "\n",
    "$B_{k+1} = B_k + y_ky_k^T/(y_k^T s_k) − (B_ks_k)(B_ks_k)^T/(s_k^T B_k s_k)$\n",
    "\n",
    "$s_k = θ_k − θ_{k−1}$\n",
    "\n",
    "$y_k = g_k − g_{k−1}$\n",
    "\n",
    "(Refer below for more details on why this notation is incorrect)\n",
    "\n",
    "This is a rank-two update to the matrix, and ensures that the matrix remains positive definite (under certain restrictions on the step size). We typically start with a diagonal approximation, $B_0 = I$. Thus BFGS can be thought of as a “diagonal plus low-rank” approximation to the Hessian.\n",
    "\n",
    "Alternatively, BFGS can iteratively update an approximation to the inverse Hessian, $C_k ≈ H_k^{−1}$, as follows:\n",
    "\n",
    "$C_{k+1} = (I − s_ky_k^T/(y_k^Ts_k))$ $C_k$ $(I − y_ks^T_k/(y^T_k s_k))$ + $s_ks^T_k/(y^T_k s_k)$\n",
    "\n",
    "<b>Note: Corrections</b>\n",
    "Text-book has a mistake. With the above notation we will not be able to start the algorithm. Wikipedia has a better mention of the algorithm. [BFGS - Algorithm](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) [Quasi-Newton Method](https://en.wikipedia.org/wiki/Quasi-Newton_method#Description_of_the_method)\n",
    "\n",
    "Both of these links mention:\n",
    "\n",
    "$s_k = θ_{k+1} − θ_k$\n",
    "\n",
    "$y_k = g_{k+1} − g_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definitions:\n",
    "# yk is a CD x 1 vector. And yk_transpose is a 1 X CD vector\n",
    "# thk is a D x C vector and when we unravel the theta it should be thk.T.ravel() \n",
    "# because gradient is listed C1(D1, D2...) and C2 (D1,..) etc. and if we need to get the same order from theta \n",
    "# then we need to unravel the transpose of the gradient vectors. Finally sk is a CD x 1 matrix\n",
    "\n",
    "# Gradients is a CD x 1 vector \n",
    "# theta is a D x C vector\n",
    "# returns a CD x CD matrix\n",
    "def quasi_hessian(bk, gk, gk_1, thk, thk_1):\n",
    "    \n",
    "    yk = gk_1 - gk\n",
    "    \n",
    "    # ravel gives us a row vector. We will convert that to a column vector\n",
    "    sk = ((thk_1 - thk).T.reshape(1,cd_size)).T\n",
    "    \n",
    "    return bk + np.matmul(yk,yk.T)/np.matmul(yk.T,sk) - np.matmul(np.matmul(bk,sk),np.matmul(bk,sk).T) / np.matmul(np.matmul(sk.T,bk),sk)\n",
    "\n",
    "# returns a CD x CD matrix\n",
    "def quasi_hess_inv(ck, gk, gk_1, thk, thk_1):\n",
    "    \n",
    "    cd_size = gk.shape[0]\n",
    "    \n",
    "    idt = np.identity(gk.shape[0])\n",
    "    yk = gk_1 - gk\n",
    "    # ravel gives us a row vector. We will convert that to a column vector\n",
    "    # sk = ((thk_1 - thk).T.ravel()).T\n",
    "    sk = ((thk_1 - thk).T.reshape(1,cd_size)).T\n",
    "    \n",
    "    part0 = np.matmul(yk.T,sk)\n",
    "    part1 = idt - (np.matmul(sk,yk.T)/part0)\n",
    "    part2 = idt - (np.matmul(yk,sk.T)/part0)\n",
    "    part3 = np.matmul(sk, sk.T)/part0\n",
    "    \n",
    "    return np.matmul(np.matmul(part1,ck),part2) + part3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  5. Newton Raphson Method\n",
    "\n",
    "About the method and its working details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the textbook:\n",
    "\n",
    "Set $w_C$ = 0, to ensure identifiability, and define w = vec(W(:, 1 : C−1)) to be a D×(C−1) column vector. Therefore we set $\\theta_c$ to 0\n",
    "\n",
    "Newton’s method\n",
    "Algorithm 8.1: Newton’s method for minimizing a strictly convex function\n",
    "1. Initialize $θ_0$;\n",
    "2. for k = 1, 2, . . . until convergence do\n",
    "3. Evaluate $g_k = ∇f(θ_k)$;\n",
    "4. Evaluate $H_k = ∇^2f(θ_k)$;\n",
    "5. Solve $H_kd_k = −g_k$ for $d_k$;\n",
    "6. Use line search to find stepsize $η_k$ along $d_k$;\n",
    "7. $θ_(k+1)$ = $θ_k + η_kd_k$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Method 1 - Implementation by Calculating Hessian and Inverse directly\n",
    "\n",
    "This hits singularity and stops and hence there is no learning. The reason the code doesn't throw singularity error is because of floating point calculations and very minute errors in the inverse calculation (but it hits a singularity. You can check this with a smaller dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha =  0.01 # Learning rate as identified by eta-k in the above definition\n",
    "\n",
    "n_iterations = 100\n",
    "m =  x.shape[0] # number of data points\n",
    "n = x.shape[1] # number of features\n",
    "epsilon = 100\n",
    "delta = 0.001\n",
    "accu = []\n",
    "\n",
    "# Add a bias to each of the rows (i.e. a constant)\n",
    "x_b = np.c_[np.ones((m,1)),x]\n",
    "\n",
    "# Take a random theta to begin the run\n",
    "np.random.seed(13)\n",
    "theta = np.random.randn(n + 1,y.max()+1)\n",
    "\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    mu = calculate_mu(x_b,theta)\n",
    "\n",
    "    gradients = gradient(mu, y_train_one_hot, x_b)  \n",
    "    hess = hessian(mu,x_b)\n",
    "    \n",
    "    step = np.matmul(np.linalg.inv(hess),gradients)\n",
    "    \n",
    "    step = step / (np.max(step)/10)\n",
    "\n",
    "    # Step is a CD x 1 Matrix. It is [C1[D1,D2..,Dn], C2[D1,D2,...,Dn]...]. Reshape the CD X 1 matrix to D X C\n",
    "    # Hence the reshape will be C x D will make it to unravel the results as required\n",
    "    # And then we need to add this as a D x C matrix to theta and hence we transpose it\n",
    "    theta = theta + alpha * step.reshape(y.max()+1, n+1).T\n",
    "    \n",
    "    accu.append(check_accuracy(theta))\n",
    "\n",
    "plt.plot(range(n_iterations),accu)\n",
    "plt.title('Iterations vs Accuracy for Newton Raphson Descent - Direct Hessian Calculation')\n",
    "check_accuracy(theta)\n",
    "    \n",
    "print(check_accuracy(theta))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Choose the step with Backtracking line search\n",
    "Alpha value keeps changing as we initiate theta with some random values initally and they change in every run.\n",
    "\n",
    "[About the method - Wikipedia](https://en.wikipedia.org/wiki/Backtracking_line_search)\n",
    "\n",
    "<img src=\"./backtrackinglinesearch.png\" height=\"600\" width=\"700\"/>\n",
    "\n",
    "\n",
    "f(θ) = J(θ) = Cross Entropy Cost Function. It is given by \n",
    "\n",
    "J(θ) = −1/m $\\sum_{i=1}^{m}$ $\\sum_{k=1}^{K}$ $y_k^{(i)}$ log ($\\hat{p}_k^{(i)}$)\n",
    "\n",
    "Where \n",
    "* m is the number of data points\n",
    "* K is the number of categories\n",
    "* $y_k^{(i)}$ is equal to 1 if the target class for the ith instance is k; otherwise it is equal to 0\n",
    "\n",
    "f(θ) = -L(θ) where L(θ) is the Log-likelihood. So, f(θ) is nothing but the negative log-likelihood. It is not difficult to see why the likelihood is the given sum. We maximize the log-likelihood i.e. L(θ) and hence we will try to minimize f(θ). L(θ) proceeds from the assumption that each of the labels is independent and from there computing the probablity is pretty straight forward. You can refer Machine Learning - A Probabilistic Perspective - Murphy for more details.\n",
    "\n",
    "In this case I ignore the 1/m factor and compute the J(θ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(theta) is here the Cross entropy cost function which is implemented here. \n",
    "def cross_entropy(x_data, y_hot, theta_val):\n",
    "    \n",
    "    # M is the number of data points\n",
    "    m = x_data.shape[0]\n",
    "    c = y_hot.shape[1]\n",
    "    \n",
    "    # This is a M X C matrix\n",
    "    log_mu = np.log(calculate_mu(x_data, theta_val))\n",
    "    \n",
    "    # y_hot is also a M X C matrix. If we unravel both these matrices then we have two MC X 1 matrices\n",
    "    # whose multiplication will give us the required sum\n",
    "    \n",
    "    nc_y = y_hot.reshape(m*c,1)\n",
    "    nc_logmu = log_mu.reshape(m*c,1)\n",
    "    \n",
    "    # This works because each of the reshapes will place the first 'C' values together and \n",
    "    # we are just doing a sum-product here which is the log-likelihood computation above.\n",
    "    \n",
    "    f_val = nc_y.T.dot(nc_logmu)\n",
    "    \n",
    "    return (- f_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the backtracking line search function\n",
    "def backtrack_linesearch(x_lcl, theta_lcl, y_lcl):\n",
    "    n_iterations = 10\n",
    "    \n",
    "    n_cat = y_lcl.shape[1]\n",
    "    n_dim = x_lcl.shape[1]\n",
    "\n",
    "    alpha = 1\n",
    "    P = 0.95\n",
    "    C = 0.20\n",
    "\n",
    "    t = C * alpha\n",
    "\n",
    "    mu_lcl = calculate_mu(x_lcl, theta_lcl)\n",
    "    #error = y_lcl - h_of_xtheta\n",
    "    \n",
    "    # act_g is the gradient in the actual format\n",
    "    act_g = gradient(mu_lcl, y_lcl, x_lcl)\n",
    "    \n",
    "    # gradients is gradient reshaped to the theta format\n",
    "    gradients = act_g.reshape(n_cat, n_dim).T\n",
    "    \n",
    "    # pk is a unit vector in the direction of the gradient. Because we want to \n",
    "    # move in the direction of the gradient\n",
    "    unit_gradient = preprocessing.normalize(act_g, norm='l2')\n",
    "    ug_thetashape = unit_gradient.reshape(n_cat, n_dim).T\n",
    "\n",
    "    grad_mul_direction = np.matmul(unit_gradient.T,act_g)\n",
    "\n",
    "    currF = cross_entropy(x_lcl, y_lcl, theta_lcl + alpha * gradients)\n",
    "    curr_iter = 0\n",
    "\n",
    "    while (currF >= cross_entropy(x_lcl, y_lcl, theta_lcl) + t * grad_mul_direction) and (curr_iter <= n_iterations):\n",
    "\n",
    "        alpha = P * alpha\n",
    "\n",
    "        currF = cross_entropy(x_lcl, y_lcl, theta_lcl + alpha * ug_thetashape)\n",
    "        curr_iter = curr_iter + 1\n",
    "    \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Method 2 - Calculating using Quasi-Newton methods\n",
    "\n",
    "Algorithm is stated in detail here on Wikipedia (and the textbook as well): \n",
    "\n",
    "[BFGS - Algorithm](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) [Quasi-Newton Method](https://en.wikipedia.org/wiki/Quasi-Newton_method#Description_of_the_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha values: [1, 1, 1, 1, 0.95, 1, 0.95, 0.95, 0.95, 1, 1, 1, 1, 1, 1, 0.95, 1, 1, 1, 0.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Iterations vs Accuracy for Newton Raphson Descent - Quasi Newton Methods')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha =  0.1 # Learning rate as identified by eta-k in the above definition\n",
    "\n",
    "n_iterations = 50\n",
    "m =  x.shape[0] # number of data points\n",
    "n = x.shape[1] # number of features\n",
    "epsilon = 100\n",
    "delta = 0.001\n",
    "accu = []\n",
    "\n",
    "# Add a bias to each of the rows (i.e. a constant)\n",
    "x_b = np.c_[np.ones((m,1)),x]\n",
    "\n",
    "# Take a random theta to begin the run\n",
    "np.random.seed(13)\n",
    "thetak = np.random.randn(n + 1,y.max()+1)\n",
    "ck = np.identity((n+1)*(y.max()+1))\n",
    "\n",
    "# Category to be made zero - Let us discard this for now.\n",
    "#zero_cat = y.max()\n",
    "#thetak[:,zero_cat] = 0\n",
    "\n",
    "mu = calculate_mu(x_b,thetak)\n",
    "gradk = gradient(mu, y_train_one_hot, x_b)\n",
    "step = np.matmul(ck,gradk)\n",
    "\n",
    "# Step is a CD x 1 Matrix. It is [C1[D1,D2..,Dn], C2[D1,D2,...,Dn]...]. Reshape the CD X 1 matrix to D X C\n",
    "# Hence the reshape will be C x D will make it to unravel the results as required\n",
    "# And then we need to add this as a D x C matrix to theta and hence we transpose it\n",
    "thetakp1 = thetak - alpha * step.reshape(y.max()+1, n+1).T\n",
    "accu = []\n",
    "alp = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    mu = calculate_mu(x_b,thetakp1)\n",
    "\n",
    "    gradkp1 = gradient(mu, y_train_one_hot, x_b)  \n",
    "    ckp1 = quasi_hess_inv(ck, gradk, gradkp1, thetak, thetakp1)\n",
    "    \n",
    "    step = np.matmul(ckp1, gradkp1)\n",
    "    \n",
    "    thetak = thetakp1\n",
    "    \n",
    "    # Get the length to move using backtracking line search\n",
    "    alpha = backtrack_linesearch(x_b, thetakp1, y_train_one_hot)\n",
    "    \n",
    "    alp.append(alpha)\n",
    "    \n",
    "    thetakp1 = thetakp1 - alpha * step.reshape(y.max()+1, n+1).T\n",
    "    \n",
    "    gradk = gradkp1\n",
    "    ck = ckp1\n",
    "    accu.append(check_accuracy(thetakp1))\n",
    "\n",
    "print(\"The alpha values:\", alp)\n",
    "plt.plot(range(n_iterations),accu)\n",
    "plt.title('Iterations vs Accuracy for Newton Raphson Descent - Quasi Newton Methods')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Final Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.38 percent\n"
     ]
    }
   ],
   "source": [
    "print(\"%.2f percent\"%(check_accuracy(thetakp1)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Comparison with Plain Vanilla Gradient Descent\n",
    "\n",
    "You can refer to my other notebook that deals with gradient descent for full details on Gradient descent implementation for Softmax Regression. I will just mention the comparisons here.\n",
    "\n",
    "#### 6.1. Batch Gradient Descent\n",
    "* Final accuracy: 81.23%\n",
    "* Alpha considered: 0.5\n",
    "* Convergence achieved after: 59 iterations\n",
    "\n",
    "#### 6.2. Stochastic Gradient Descent (Using just one data point)\n",
    "* Final accuracy: 80.9%\n",
    "* Nature: Keep fluctuating\n",
    "* Convergence achieved after: 900 iterations\n",
    "* But even after that it is not stable. Keeps moving around\n",
    "\n",
    "#### 6.3. Mini-Batch Gradient Descent\n",
    "<b>Size: 16</b>\n",
    "* Final accuracy: 79.69%\n",
    "* Convergence achieved after: 177 iterations\n",
    "* Fluctuations are lower. Reaches some stability after 177 iterations\n",
    "\n",
    "<b>Size: 32</b>\n",
    "* Final accuracy: 79.38%\n",
    "* Convergence achieved after: 125 iterations\n",
    "* Fluctuations are lower. Reaches some stability after 125 iterations\n",
    "\n",
    "#### 6.4. Newton Raphson Method (Pseudo Newton Methods & With Backtracking Linesearch)\n",
    "* Final accuracy: 83.38%\n",
    "* Convergence achieved after: 8 iterations\n",
    "* Almost no major fluctuation in accuracy even on large number of runs\n",
    "\n",
    "#### 6.5. Summary \n",
    "The quickness with which Newton Raphson converges is simply awesome. But computationally and memory usage wise it is horrible. Just look at the order of storage and computations we used above and it should be clear. There is a better method called L-BFGS which uses lesser memory than this Quasi-method uses now (But I am am not pursuing it further in this notebook)\n",
    "\n",
    "<img src=\"./run_comparisons.png\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
